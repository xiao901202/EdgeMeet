# ---------------------------------------------------------------------
# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# ---------------------------------------------------------------------
from __future__ import annotations

import os

import numpy as np
import torch
import torch.nn as nn

from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, SourceAsRoot
from qai_hub_models.utils.base_model import BaseModel
from qai_hub_models.utils.input_spec import InputSpec

SIMPLEBEV_SOURCE_REPOSITORY = "https://github.com/aharley/simple_bev"
SIMPLEBEV_SOURCE_REPO_COMMIT = "be46f0ef71960c233341852f3d9bc3677558ab6d"
SIMPLEBEV_SOURCE_PATCHES = [
    os.path.abspath(
        os.path.join(os.path.dirname(__file__), "patches", "simple_bev_patch.diff")
    )
]
MODEL_ID = __name__.split(".")[-2]
MODEL_ASSET_VERSION = 1
DEFAULT_WEIGHTS = "model-000025000.pth"

with SourceAsRoot(
    SIMPLEBEV_SOURCE_REPOSITORY,
    SIMPLEBEV_SOURCE_REPO_COMMIT,
    MODEL_ID,
    MODEL_ASSET_VERSION,
    source_repo_patches=SIMPLEBEV_SOURCE_PATCHES,
):
    import utils.vox
    from nets.segnet import Segnet

    Z, Y, X = 200, 8, 200
    scene_centroid_x = 0.0
    scene_centroid_y = 1.0
    scene_centroid_z = 0.0
    xmin, xmax = -50, 50
    zmin, zmax = -50, 50
    ymin, ymax = -5, 5


class SimpleBev(BaseModel):
    def __init__(
        self,
        model: nn.Module,
        use_radar: bool = False,
        use_lidar: bool = False,
        use_metaradar: bool = False,
        do_rgbcompress: bool = True,
        encoder_type: str = "res101",
        device: str = "cpu",
    ) -> None:
        super().__init__()
        self.model = model
        self.scene_centroid_x = scene_centroid_x
        self.scene_centroid_y = scene_centroid_y
        self.scene_centroid_z = scene_centroid_z
        self.xmin, self.xmax = xmin, xmax
        self.zmin, self.zmax = zmin, zmax
        self.ymin, self.ymax = ymin, ymax
        self.bounds = (self.xmin, self.xmax, self.ymin, self.ymax, self.zmin, self.zmax)
        self.Z, self.Y, self.X = Z, Y, X
        self.scene_centroid_py = np.array(
            [
                self.scene_centroid_x,
                self.scene_centroid_y,
                self.scene_centroid_z,
            ]
        ).reshape([1, 3])
        self.scene_centroid = torch.from_numpy(self.scene_centroid_py).float()
        self.vox_util_ = utils.vox.Vox_util(
            self.Z,
            self.Y,
            self.X,
            scene_centroid=self.scene_centroid.to("cpu"),
            bounds=self.bounds,
            assert_cube=False,
        )

        self.use_radar = use_radar
        self.use_lidar = use_lidar
        self.use_metaradar = use_metaradar
        self.do_rgbcompress = do_rgbcompress
        self.encoder_type = encoder_type
        self.device = device

    @classmethod
    def from_pretrained(
        cls,
        weights: str = DEFAULT_WEIGHTS,
        use_radar: bool = False,
        use_lidar: bool = False,
        use_metaradar: bool = False,
        do_rgbcompress: bool = True,
        encoder_type: str = "res101",
        device: str = "cpu",
    ):
        checkpoint_path = CachedWebModelAsset.from_asset_store(
            MODEL_ID, MODEL_ASSET_VERSION, weights
        ).fetch()
        simple_bev_model = _load_model_from_weight(
            checkpoint_path,
            use_radar,
            use_lidar,
            use_metaradar,
            do_rgbcompress,
            encoder_type,
            device,
        )
        model_cls = cls(simple_bev_model)
        return model_cls

    def forward(
        self,
        rgb_camXs: torch.Tensor,
        pix_T_cams: torch.Tensor,
        cam0_T_camXs: torch.Tensor,
    ):
        rgb_camXs = rgb_camXs
        pix_T_cams = pix_T_cams
        cam0_T_camXs = cam0_T_camXs
        """
        Run Segnet model and return the transformed and segmented object data for generating a 2D bird's eye view(BEV) of the environment around the  vehicle

        Parameters:
            B = batch size, S = number of cameras, C = 3, H = img height, W = img width
            rgb_camXs: torch.Tensor of shape [B,S,C,H,W]
                Image data generated by each camera sensor
            pix_T_cams: torch.Tensor of shape [B,S,4,4]
                Camera intrinsic matrix - Used for Projecting 2D image to 3D grid plane
            cam0_T_camXs: torch.Tensor of shape [B,S,4,4]
                Transformation matrices from various cameras (camXs) to reference camera (camera0) coordinate

        Returns:
            raw_e_results : Raw feature data:  torch.Tensor of shape (1,k,Z,X)
            feat_e_results: Processed feature data: torch.Tensor of shape (1,k,Z,X)
            seg_e_results: Segmented objects (foreground/background):   torch.Tensor of shape (1,1,Z,X)
            center_e_results:Position (1D) of each segmented object. torch.Tensor of shape (1,1,Z,X)
            offset_e_results:Offset position (2D) of each segmented object. torch.Tensor of shape (1,2,Z,X)

        Where,
            k = number of feature points
        """
        return self.model(rgb_camXs, pix_T_cams, cam0_T_camXs, self.vox_util_)

    @staticmethod
    def get_input_spec(
        batch_size: int = 1,
        height: int = 224 * 2,
        width: int = 400 * 2,
    ) -> InputSpec:

        """
        Returns the input specification (name -> (shape, type). This can be
        used to submit profiling job on Qualcomm AI Hub.
        """
        return {
            "rgb_camXs": ((batch_size, 6, 3, height, width), "float32"),
            "pix_T_cams": ((batch_size, 6, 4, 4), "float32"),
            "cam0_T_camXs": ((batch_size, 6, 4, 4), "float32"),
        }

    @staticmethod
    def get_output_names() -> list[str]:
        return [
            "raw_e_results",
            "feat_e_results",
            "seg_e_results",
            "center_e_results",
            "offset_e_results",
        ]

    @staticmethod
    def get_channel_last_inputs() -> list[str]:
        return ["cam0_T_camXs"]

    @staticmethod
    def get_channel_last_outputs() -> list[str]:
        return ["offset_e_results"]


def _load_model_from_weight(
    checkpoint_path,
    use_radar,
    use_lidar,
    use_metaradar,
    do_rgbcompress,
    encoder_type,
    device,
):

    bounds = (xmin, xmax, ymin, ymax, zmin, zmax)
    scene_centroid_py = np.array(
        [scene_centroid_x, scene_centroid_y, scene_centroid_z]
    ).reshape([1, 3])
    scene_centroid = torch.from_numpy(scene_centroid_py).float()
    scene_centroid = scene_centroid.to("cpu")
    vox_util = utils.vox.Vox_util(
        Z, Y, X, scene_centroid, bounds, None, assert_cube=False
    )
    model = Segnet(
        Z,
        Y,
        X,
        vox_util,
        use_radar=use_radar,
        use_lidar=use_lidar,
        use_metaradar=use_metaradar,
        do_rgbcompress=do_rgbcompress,
        encoder_type=encoder_type,
    )
    checkpoint = torch.load(checkpoint_path, weights_only=True, map_location=device)[
        "model_state_dict"
    ]

    model_dict = model.state_dict()

    # 1. filter out ignored keys
    pretrained_dict = {k: v for k, v in checkpoint.items()}

    # 2. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)

    # 3. load the new state dict
    model.load_state_dict(model_dict, strict=False)
    model.to("cpu").eval()
    return model
