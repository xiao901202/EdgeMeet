diff --git a/src/transformers/models/mask2former/modeling_mask2former.py b/src/transformers/models/mask2former/modeling_mask2former.py
index e4fba109a0..b524889cd2 100644
--- a/src/transformers/models/mask2former/modeling_mask2former.py
+++ b/src/transformers/models/mask2former/modeling_mask2former.py
@@ -807,10 +807,13 @@ def multi_scale_deformable_attention(
     attention_weights: Tensor,
 ) -> Tensor:
     batch_size, _, num_heads, hidden_dim = value.shape
-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
+    # _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
+    num_queries, num_heads, num_points, _ = sampling_locations.shape
+    num_queries //= batch_size
     value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)
     sampling_grids = 2 * sampling_locations - 1
     sampling_value_list = []
+    sampling_grids = sampling_grids.split(num_points//len(value_spatial_shapes),2)
     for level_id, (height, width) in enumerate(value_spatial_shapes):
         # batch_size, height*width, num_heads, hidden_dim
         # -> batch_size, height*width, num_heads*hidden_dim
@@ -822,7 +825,8 @@ def multi_scale_deformable_attention(
         # batch_size, num_queries, num_heads, num_points, 2
         # -> batch_size, num_heads, num_queries, num_points, 2
         # -> batch_size*num_heads, num_queries, num_points, 2
-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)
+        # sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)
+        sampling_grid_l_ = sampling_grids[level_id].transpose(0,1)
         # batch_size*num_heads, hidden_dim, num_queries, num_points
         sampling_value_l_ = nn.functional.grid_sample(
             value_l_, sampling_grid_l_, mode="bilinear", padding_mode="zeros", align_corners=False
@@ -832,10 +836,12 @@ def multi_scale_deformable_attention(
     # -> (batch_size, num_heads, num_queries, num_levels, num_points)
     # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)
     attention_weights = attention_weights.transpose(1, 2).reshape(
-        batch_size * num_heads, 1, num_queries, num_levels * num_points
+        # batch_size * num_heads, 1, num_queries, num_levels * num_points
+        batch_size * num_heads, 1, num_queries, num_points
     )
     output = (
-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)
+        # (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)
+        (torch.concat(sampling_value_list, dim=-1) * attention_weights)
         .sum(-1)
         .view(batch_size, num_heads * hidden_dim, num_queries)
     )
@@ -948,24 +954,33 @@ class Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):
             value = value.masked_fill(attention_mask[..., None], float(0))
         value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)
         sampling_offsets = self.sampling_offsets(hidden_states).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2
+            # batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2
+            batch_size* num_queries, self.n_heads, self.n_levels* self.n_points, 2
         )
         attention_weights = self.attention_weights(hidden_states).view(
             batch_size, num_queries, self.n_heads, self.n_levels * self.n_points
         )
-        attention_weights = nn.functional.softmax(attention_weights, -1).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
-        )
+        attention_weights = nn.functional.softmax(attention_weights, -1)#.view(
+        #     batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
+        # )
         # batch_size, num_queries, n_heads, n_levels, n_points, 2
         if reference_points.shape[-1] == 2:
             offset_normalizer = torch.tensor(
                 [[shape[1], shape[0]] for shape in spatial_shapes_list],
-                dtype=torch.long,
+                dtype=torch.int32,
                 device=reference_points.device,
             )
+
+            # sampling_locations = (
+            #     reference_points[:, :, None, :, None, :]
+            #     + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+            # )
+            reference_points = reference_points.reshape(-1,self.n_levels,1, 2).repeat(1,1,self.n_points,1).reshape(-1,1,self.n_levels*self.n_points,2)
             sampling_locations = (
-                reference_points[:, :, None, :, None, :]
-                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+                # reference_points[:, :, None, :, None, :]
+                # + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+                reference_points
+                + sampling_offsets / offset_normalizer.unsqueeze(-2).repeat(1,self.n_points,1).reshape(1,1,-1,2)
             )
         elif reference_points.shape[-1] == 4:
             sampling_locations = (
@@ -1886,6 +1901,7 @@ class Mask2FormerMaskedAttentionDecoder(nn.Module):
             else:
                 level_index = idx % self.num_feature_levels
 
+                # attention_mask[torch.where(attention_mask.sum(-1) == attention_mask.shape[-1])] = False
                 where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)
                 # Multiply the attention mask instead of indexing to avoid issue in torch.export.
                 attention_mask = attention_mask * where.unsqueeze(-1)
@@ -2020,16 +2036,18 @@ class Mask2FormerMaskPredictor(nn.Module):
 
         is_tracing = torch.jit.is_tracing() or isinstance(outputs, torch.fx.Proxy) or is_torchdynamo_compiling()
         # Sum up over the channels
-        if is_tracing and not is_torch_greater_or_equal_than_2_1:
-            # Equivalent to einsum('bqc, bchw -> bqhw') but jit friendly
-            batch_size, num_queries, num_channels = mask_embeddings.shape
-            _, _, height, width = pixel_embeddings.shape
-            outputs_mask = torch.zeros((batch_size, num_queries, height, width), device=mask_embeddings.device)
-            for c in range(num_channels):
-                outputs_mask += mask_embeddings[..., c][..., None, None] * pixel_embeddings[:, None, c]
-
-        else:
-            outputs_mask = torch.einsum("bqc, bchw -> bqhw", mask_embeddings, pixel_embeddings)
+        # if is_tracing and not is_torch_greater_or_equal_than_2_1:
+        # Equivalent to einsum('bqc, bchw -> bqhw') but jit friendly
+        batch_size, num_queries, num_channels = mask_embeddings.shape
+        _, _, height, width = pixel_embeddings.shape
+        # outputs_mask = torch.zeros((batch_size, num_queries, height, width), device=mask_embeddings.device)
+        # for c in range(num_channels):
+        #     outputs_mask += mask_embeddings[..., c][..., None, None] * pixel_embeddings[:, None, c]
+        pixel_embeddings = pixel_embeddings.reshape(batch_size,num_channels,-1)
+        outputs_mask = torch.matmul(mask_embeddings, pixel_embeddings).reshape(batch_size,num_queries,height, width)
+
+        # else:
+        #     outputs_mask = torch.einsum("bqc, bchw -> bqhw", mask_embeddings, pixel_embeddings)
 
         attention_mask = nn.functional.interpolate(
             outputs_mask, size=attention_mask_target_size, mode="bilinear", align_corners=False
diff --git a/src/transformers/models/swin/modeling_swin.py b/src/transformers/models/swin/modeling_swin.py
index 5de428831e..c58f3e0e6e 100644
--- a/src/transformers/models/swin/modeling_swin.py
+++ b/src/transformers/models/swin/modeling_swin.py
@@ -215,9 +215,11 @@ def window_partition(input_feature, window_size):
     """
     batch_size, height, width, num_channels = input_feature.shape
     input_feature = input_feature.view(
-        batch_size, height // window_size, window_size, width // window_size, window_size, num_channels
+        # batch_size, height // window_size, window_size, width // window_size, window_size, num_channels
+        batch_size*height // window_size, window_size, width // window_size, window_size*num_channels
     )
-    windows = input_feature.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)
+    # windows = input_feature.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)
+    windows = input_feature.permute(0, 2, 1, 3).contiguous().view(-1, window_size, window_size, num_channels)
     return windows
 
 
@@ -226,8 +228,10 @@ def window_reverse(windows, window_size, height, width):
     Merges windows to produce higher resolution features.
     """
     num_channels = windows.shape[-1]
-    windows = windows.view(-1, height // window_size, width // window_size, window_size, window_size, num_channels)
-    windows = windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, height, width, num_channels)
+    # windows = windows.view(-1, height // window_size, width // window_size, window_size, window_size, num_channels)
+    # windows = windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, height, width, num_channels)
+    windows = windows.view(-1, width // window_size, window_size, window_size*num_channels)
+    windows = windows.permute(0, 2, 1, 3).contiguous().view(-1, height, width, num_channels)
     return windows
 
 
@@ -530,10 +534,11 @@ class SwinSelfAttention(nn.Module):
             # Apply the attention mask is (precomputed for all layers in SwinModel forward() function)
             mask_shape = attention_mask.shape[0]
             attention_scores = attention_scores.view(
-                batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim
+                # batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim
+                -1, self.num_attention_heads, dim, dim
             )
-            attention_scores = attention_scores + attention_mask.unsqueeze(1).unsqueeze(0)
-            attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
+            attention_scores = attention_scores + attention_mask.unsqueeze(1)#.unsqueeze(0)
+            # attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
         attention_probs = nn.functional.softmax(attention_scores, dim=-1)
