# ---------------------------------------------------------------------
# Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# ---------------------------------------------------------------------
from __future__ import annotations

from enum import Enum
from typing import Callable

import numpy as np
import torch
import torch.nn as nn
from PIL.Image import Image
from torchvision.transforms import Resize

from qai_hub_models.models.sam2.model_patches import SAM2ImagePredictor, SAM2Transforms
from qai_hub_models.models.sam2.model_patches import (
    mask_postprocessing as upscale_masks,
)
from qai_hub_models.utils.image_processing import (
    numpy_image_to_torch,
    preprocess_PIL_image,
)


class SAM2InputImageLayout(Enum):
    RGB = 0
    BGR = 1


class PrepPromptContext:

    """
    A context class to provide necessary attributes for _prep_prompts.

    Attributes:
        _transforms: Transformations applied to the input image.
        _orig_hw: Original height and width of the input image.
        device: Device on which processing will occur (default is 'cpu').
    """

    def __init__(
        self,
        image_size: int,
        mask_threshold: float,
        input_images_original_size: tuple[int, int],
        device: str = "cpu",
    ) -> None:
        self._transforms = SAM2Transforms(image_size, mask_threshold)
        self._orig_hw = [
            [
                input_images_original_size[0],
                input_images_original_size[1],
            ]
        ]
        self.device = device


class SAM2App:
    """
    This class consists of light-weight "app code" that is required to perform end to end inference with Segment-Anything-2 Model.

    The app uses 2 models:
        * encoder (Given input image, emits image embeddings, high-resolution features to be used by decoder)
        * decoder (image embeddings and high-resolution features --> predicted segmentation masks and scores)
    """

    def __init__(
        self,
        encoder_input_img_size: int,
        mask_threshold: float,
        input_image_channel_layout: SAM2InputImageLayout,
        sam2_encoder: Callable[
            [torch.Tensor], tuple[torch.Tensor, torch.Tensor, torch.Tensor]
        ],
        sam2_decoder: Callable[
            [torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],
            tuple[torch.Tensor, torch.Tensor],
        ],
    ) -> None:

        """
        Initializes the segmentation model with encoder and decoder components.

        Args:
            mask_threshold (float): Threshold value used to binarize the predicted masks.

            input_image_channel_layout: SAMInputImageLayout
                Channel layout ("RGB" or "BGR") expected by the encoder.

            sam2_encoder (Callable):
                SAM2 encoder. Must match input & output of each model part generated by qai_hub_models.models.sam2.model.SAM2Encoder
                Takes image to produce the image embeddings and high-resolutions features for sam2 decoder

            sam2_decoder (Callable):
                SAM2 decoder. Must match input and output of qai_hub_models.models.sam2.model.SAM2Decoder.
                Takes image embeddings and high-resolution features to produce segmentation masks and confidence scores.
        """

        self.sam2_encoder = sam2_encoder
        self.sam2_decoder = sam2_decoder
        self.mask_threshold = mask_threshold
        self.encoder_input_img_size = encoder_input_img_size
        self.box = None
        self.normalize_coords = True
        self.mask_input = None
        self.input_image_channel_layout = input_image_channel_layout

    def predict(self, *args, **kwargs):
        return self.predict_mask_from_points(*args, **kwargs)

    def predict_mask_from_points(
        self,
        pixel_values_or_image: torch.Tensor | np.ndarray | Image | list[Image],
        point_coords: torch.Tensor,
        point_labels: torch.Tensor,
        return_logits: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor]:

        """
        Predicts segmentation masks from input image(s) and point-based prompts.

        This function combines the embedding generation and mask prediction steps.
        It first encodes the input image(s) to obtain image embeddings and high-resolution
        features, then uses the provided point coordinates and labels to predict segmentation masks.

        Parameters:
            pixel_values_or_image: torch.Tensor
                PIL image
                or
                numpy array (N H W C x uint8) or (H W C x uint8)
                    channel layout consistent with self.input_image_channel_layout
                or
                pyTorch tensor (N C H W x int8, value range is [0, 255])
                    channel layout consistent with self.input_image_channel_layout
            point_coords (torch.Tensor): Coordinates of points used as prompts (shape: [N, 2] or [B, N, 2]).
            point_labels (torch.Tensor): Labels for the points (shape: [N] or [B, N]).
            return_logits (bool, optional): If True, return raw logits; otherwise, apply thresholding. Defaults to False.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:
                - upscaled_masks: torch.Tensor of shape [b, 1, <input image spatial dims>]
                    The predicted segmentation masks, upscaled to original size.
                - scores: torch.Tensor of shape [b, 1]
                    Confidence scores for each predicted mask.

        Where,
            b = number of input images
        """
        (
            image_embeddings,
            high_res_features1,
            high_res_features2,
            input_images_original_size,
        ) = self.predict_embeddings(pixel_values_or_image)
        return self.predict_mask_from_points_and_embeddings(
            image_embeddings,
            high_res_features1,
            high_res_features2,
            point_coords,
            point_labels,
            input_images_original_size,
            return_logits,
        )

    def predict_embeddings(
        self, pixel_values_or_image: torch.Tensor | np.ndarray | Image | list[Image]
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, tuple[int, int]]:

        """
        Generates image embeddings and high-resolution features from input image(s).

        This function processes the input image(s) through a transformation pipeline and
        a series of encoder splits to produce image embeddings and high-resolution features
        required for downstream tasks such as segmentation.

        Parameters:
            pixel_values_or_image: torch.Tensor
                PIL image
                or
                numpy array (N H W C x uint8) or (H W C x uint8)
                    channel layout consistent with self.input_image_channel_layout
                or
                pyTorch tensor (N C H W x int8, value range is [0, 255])
                    channel layout consistent with self.input_image_channel_layout

        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, list[int]]:
                - image_embeddings (torch.Tensor) [1,256,64,64]: The image embeddings from the encoder.
                - high_res_features1 (torch.Tensor) [1, 32, 256, 256]: First set of high-resolution features.
                - high_res_features2 (torch.Tensor) [1, 64, 128, 128]: Second set of high-resolution features.
                - input_images_original_size: tuple[int, int]: Original size of input image (BEFORE reshape to fit encoder input size)

        Discussion:
            It is faster to run this once on an image (compared to the entire encoder / decoder pipeline)
            if masks will be predicted several times on the same image.
        """
        # Translate input to torch tensor of shape [N, C, H, W]
        if isinstance(pixel_values_or_image, Image):
            pixel_values_or_image = [pixel_values_or_image]
        if isinstance(pixel_values_or_image, list):
            NCHW_int8_torch_frames = torch.cat(
                [
                    preprocess_PIL_image(
                        x.convert(self.input_image_channel_layout.name), True
                    )
                    for x in pixel_values_or_image
                ]
            )
        elif isinstance(pixel_values_or_image, np.ndarray):
            NCHW_int8_torch_frames = numpy_image_to_torch(pixel_values_or_image, True)
        else:
            NCHW_int8_torch_frames = pixel_values_or_image

        # Resize input image to the encoder's desired input size.
        input_images_original_size = (
            NCHW_int8_torch_frames.shape[2],
            NCHW_int8_torch_frames.shape[3],
        )
        # Run encoder
        image = self.resize_image(NCHW_int8_torch_frames, self.encoder_input_img_size)
        image_embeddings, high_res_features1, high_res_features2 = self.sam2_encoder(
            image
        )

        return (
            image_embeddings,
            high_res_features1,
            high_res_features2,
            input_images_original_size,
        )

    def predict_mask_from_points_and_embeddings(
        self,
        image_embeddings: torch.Tensor,
        high_res_features1: torch.Tensor,
        high_res_features2: torch.Tensor,
        point_coords: torch.Tensor,
        point_labels: torch.Tensor,
        input_images_original_size: tuple[int, int],
        return_logits: bool = False,
    ) -> tuple[torch.Tensor, torch.Tensor]:

        """
        Predicts segmentation masks from image embeddings and point-based prompts.

        This function takes image embeddings, high-resolution features, and user-provided
        point coordinates and labels to generate segmentation masks using a decoder.
        The resulting masks are upscaled to the original image size and optionally thresholded.

        Parameters:
            image_embeddings (torch.Tensor) [1,256,64,64]: The image embeddings from the encoder.
            high_res_features1 (torch.Tensor) [1, 32, 256, 256]: First set of high-resolution features.
            high_res_features2 (torch.Tensor) [1, 64, 128, 128]: Second set of high-resolution features.
            point_coords (torch.Tensor): Coordinates of points used as prompts (shape: [N, 2] or [B, N, 2]).
            point_labels (torch.Tensor): Labels for the points (shape: [N] or [B, N]).
            input_images_original_size (tuple[int, int]): Original size of the input image (height, width).
            input_images_original_size: tuple[int, int]: Original size of input image (BEFORE reshape to fit encoder input size)
            return_logits (bool): If True, return raw logits; otherwise, apply thresholding.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:
                - upscaled_masks: torch.Tensor of shape [b, 1, <input image spatial dims>]
                    The predicted segmentation masks, upscaled to original size.
                - scores: torch.Tensor of shape [b, 1]
                    Confidence scores for each predicted mask.

        Where,
            b = number of input images
        """
        # Expand point_coords and point_labels to include a batch dimension, if necessary
        if len(point_coords.shape) == 2:
            point_coords = torch.unsqueeze(point_coords, 0)
        if len(point_labels.shape) == 1:
            point_labels = torch.unsqueeze(point_labels, 0)

        # Create a context object with required attributes to call _prep_prompts method
        prep_prompt_context = PrepPromptContext(
            self.encoder_input_img_size, self.mask_threshold, input_images_original_size
        )
        # Change point coordinates to map to the same pixel in the resized image.
        _, unnorm_coords, labels, _ = SAM2ImagePredictor._prep_prompts(
            prep_prompt_context,
            point_coords,
            point_labels,
            self.box,
            self.mask_input,
            self.normalize_coords,
        )

        # Run decoder
        masks, scores = self.sam2_decoder(
            image_embeddings,
            high_res_features1,
            high_res_features2,
            unnorm_coords,
            labels,
        )

        # Upscale masks
        upscaled_masks = upscale_masks(masks, input_images_original_size)

        # Apply mask threshold
        if not return_logits:
            upscaled_masks = upscaled_masks > self.mask_threshold

        return upscaled_masks, scores

    def resize_image(self, x: torch.Tensor, resolution: int) -> torch.Tensor:
        """
        Resize an image tensor or PIL image to the given resolution using torchvision and torchscript.

        Args:
            x: Input image (torch.Tensor).
            resolution: Target resolution (int) for both height and width.

        Returns:
            torch.Tensor: Resized image tensor.
        """
        resize_transform = torch.jit.script(
            nn.Sequential(Resize((resolution, resolution)))
        )

        return resize_transform(x)
