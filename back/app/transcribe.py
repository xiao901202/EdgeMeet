from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse
from typing import List, Dict, Any, Tuple
from pathlib import Path
import os
import math
import json
import wave
import numpy as np
from io import BytesIO

# éŸ³è¨Šè™•ç†ï¼ˆéœ€ ffmpeg æˆ– avlibï¼‰ï¼špip install pydub
from pydub import AudioSegment

# AI æ¨¡å‹ç›¸é—œ imports
from kuwa.client import KuwaClient
from qai_hub_models.models.whisper_large_v3_turbo.model import WhisperLargeV3Turbo
from qai_hub_models.models._shared.hf_whisper.app import HfWhisperApp

router = APIRouter()

UPLOAD_DIR = Path("./uploads").resolve()
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# ===== è¦æ ¼ =====
CHUNK_SECONDS = 20          # æ¯æ®µé•·åº¦
OVERLAP_SECONDS = 2         # èˆ‡å‰æ®µé‡ç–Š
TARGET_SR = 16000           # å»ºè­°è½‰éŒ„å–æ¨£ç‡
TARGET_CH = 1               # å–®è²é“
TARGET_BITS = 16            # 16-bit PCM

# ===== AI æ¨¡å‹åƒæ•¸ =====
VOLUME_THRESHOLD = 3        # éŸ³é‡é–¾å€¼
SUMMARY_BATCH_SIZE = 3      # æ¯3æ®µåšä¸€æ¬¡æ‘˜è¦

# ===== æª”å/è³‡æ–™å¤¾ =====
TRANSCRIPT_JSON = "transcript.json"  # é€æ®µæ¸…å–®ï¼ˆå« start/end/textï¼‰
SUMMARY_JSON   = "summary.json"      # æ•´é«”æ‘˜è¦ +ï¼ˆå¯é¸ï¼‰é€æ®µæ‘˜è¦
FULL_WAV       = "base.wav"          # ä¼ºæœç«¯è¦ä¸€åŒ–å¾Œçš„å®Œæ•´ WAV
CHUNK_DIRNAME  = "chunks"            # ç‰‡æ®µè³‡æ–™å¤¾ï¼ˆ001.wav, 002.wav, ...ï¼‰
STREAM_CHUNKS  = "stream_chunks"     # ä¸²æµä¸Šå‚³æš«å­˜çš„ 20s åˆ†æ®µï¼ˆ001.wav, 002.wav, ...ï¼‰

# ===== ç‹€æ…‹æ¨™è¨˜ =====
PROCESSING_TEXT = "è™•ç†ä¸­..."
PROCESSING_SUMMARY = "è™•ç†ä¸­..."

# ===== å…¨åŸŸ AI æ¨¡å‹å¯¦ä¾‹ =====
whisper_app = None
kuwa_client = None

def init_ai_models():
    """åˆå§‹åŒ– AI æ¨¡å‹"""
    global whisper_app, kuwa_client
    try:
        print("æ­£åœ¨è¼‰å…¥ Whisper æ¨¡å‹...")
        model = WhisperLargeV3Turbo.from_pretrained()
        whisper_app = HfWhisperApp(model)
        print("âœ… Whisper æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # åˆå§‹åŒ– KuwaClient
        kuwa_client = KuwaClient(
            base_url="http://127.0.0.1",
            model=".bot/TAIDE LX 8B",
            auth_token=os.environ.get("KUWA_API_KEY")
        )
        print("âœ… KuwaClient åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ AI æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        raise e

# åœ¨æ¨¡çµ„è¼‰å…¥æ™‚åˆå§‹åŒ–æ¨¡å‹
try:
    init_ai_models()
except Exception as e:
    print(f"è­¦å‘Šï¼šAI æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼: {e}")
    whisper_app = None
    kuwa_client = None


# ===============================
# å·¥å…·/è¼”åŠ©
# ===============================
def _canonicalize_bytes_to_wav16k_mono(contents: bytes, out_wav: Path, src_ext: str) -> Path:
    """
    å°‡ä¸Šå‚³ bytesï¼ˆä¾å‰¯æª”åæ¨æ ¼å¼ï¼‰ç›´æ¥è½‰æˆ 16k/mono/16bit WAVï¼Œè¼¸å‡ºåˆ° out_wavã€‚
    ä¸åœ¨ç£ç¢Ÿä¸Šç•™ä¸‹åŸå§‹æª”ã€‚
    """
    ext = (src_ext or "").lstrip(".").lower() or None
    audio = AudioSegment.from_file(BytesIO(contents), format=ext)
    audio = audio.set_frame_rate(TARGET_SR).set_channels(TARGET_CH).set_sample_width(TARGET_BITS // 8)
    audio.export(out_wav.as_posix(), format="wav")
    return out_wav


def _ensure_folder(base_name: str) -> Path:
    folder = UPLOAD_DIR / base_name
    folder.mkdir(parents=True, exist_ok=True)
    return folder


def _index_to_times(i: int) -> Tuple[float, float]:
    """ç¬¬ i æ®µï¼ˆ1-basedï¼‰çš„ [start, end) ç§’æ•¸ï¼ˆå« 2 ç§’é‡ç–Šé‚è¼¯çš„æ­¥é€²ï¼‰ã€‚"""
    step = CHUNK_SECONDS - OVERLAP_SECONDS
    start = (i - 1) * step
    end   = start + CHUNK_SECONDS
    return float(start), float(end)


def _time_to_index(t: float, total_seconds: float) -> int:
    """æ™‚é–“ tï¼ˆç§’ï¼‰å°æ‡‰åˆ°ç¬¬å¹¾æ®µï¼ˆ1-basedï¼‰ã€‚"""
    if t < 0: t = 0.0
    step = CHUNK_SECONDS - OVERLAP_SECONDS
    idx = int(t // step) + 1
    # ä¸Šé™ä¿è­·ï¼ˆä»¥æœ€å¾Œä¸€æ®µ end ç•¥ä¼°ç¸½é•·ï¼‰
    max_idx = max(1, math.ceil((max(0.0, total_seconds) - OVERLAP_SECONDS) / step))
    return min(idx, max_idx)


def check_audio_volume(wav_path: Path) -> float:
    """æª¢æŸ¥éŸ³é‡å¼·åº¦"""
    try:
        with wave.open(str(wav_path), 'rb') as wf:
            frames = wf.readframes(wf.getnframes())
            audio_data = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32767.0
            volume = np.linalg.norm(audio_data)
            return float(volume)  # æ˜ç¢ºè½‰æ›ç‚º Python åŸç”Ÿ float
    except Exception as e:
        print(f"âŒ ç„¡æ³•è®€å–éŸ³è¨Šæª”æ¡ˆ {wav_path}: {e}")
        return 0.0


def _init_json_files(folder: Path, total_estimated_segments: int = 0) -> Tuple[Path, Path]:
    """åˆå§‹åŒ– JSON æª”æ¡ˆï¼Œé å…ˆå‰µå»ºå¸¶æœ‰è™•ç†ä¸­ç‹€æ…‹çš„çµæ§‹"""
    tr_path = folder / TRANSCRIPT_JSON
    sm_path = folder / SUMMARY_JSON
    
    # åˆå§‹åŒ– transcript.json
    if not tr_path.exists():
        tr_data = {
            "base_name": folder.name,
            "chunk_seconds": CHUNK_SECONDS,
            "overlap_seconds": OVERLAP_SECONDS,
            "segments": []
        }
        
        # å¦‚æœçŸ¥é“é ä¼°æ®µæ•¸ï¼Œå¯ä»¥é å…ˆå‰µå»ºä½”ä½ç¬¦
        if total_estimated_segments > 0:
            for i in range(1, total_estimated_segments + 1):
                start, end = _index_to_times(i)
                tr_data["segments"].append({
                    "index": i,
                    "start": start,
                    "end": end,
                    "text": PROCESSING_TEXT,
                    "summary": PROCESSING_SUMMARY,  # transcript ä¸­æš«æ™‚ä¿ç•™ summary æ¬„ä½
                    "volume": 0.0
                })
        
        _write_json(tr_path, tr_data)
    
    # summary.json å°‡åœ¨ overall æ‘˜è¦å®Œæˆå¾Œæ‰å‰µå»º
    return tr_path, sm_path


async def transcribe_with_whisper(chunk_path: Path, idx: int) -> Dict[str, Any]:
    """ä½¿ç”¨çœŸå¯¦çš„ Whisper æ¨¡å‹é€²è¡Œè½‰éŒ„"""
    start, end = _index_to_times(idx)
    
    # æª¢æŸ¥éŸ³é‡
    volume = check_audio_volume(chunk_path)
    print(f"ğŸ“¶ ç¬¬ {idx:03d} æ®µéŸ³é‡: {volume:.4f}")
    
    if volume < VOLUME_THRESHOLD:
        print(f"ğŸ”‡ ç¬¬ {idx:03d} æ®µéŸ³é‡éä½ï¼ˆ<{VOLUME_THRESHOLD}ï¼‰ï¼Œè·³éè½‰éŒ„")
        text = ""
    else:
        try:
            if whisper_app:
                print(f"ğŸ§ é–‹å§‹è½‰éŒ„ç¬¬ {idx:03d} æ®µ: {chunk_path.name}")
                text = whisper_app.transcribe(str(chunk_path))
                print(f"ç¬¬ {idx:03d} æ®µè½‰éŒ„çµæœ: {text}")
            else:
                # æ¨¡æ“¬æ¨¡å¼
                text = f"ï¼ˆæ¨¡æ“¬ï¼‰ç¬¬ {idx:03d} æ®µçš„è½‰éŒ„æ–‡å­—ï¼ˆ{start:.1f}s ~ {end:.1f}sï¼‰ã€‚"
                
        except Exception as e:
            print(f"âŒ è½‰éŒ„ç¬¬ {idx:03d} æ®µå¤±æ•—: {e}")
            text = f"è½‰éŒ„å¤±æ•—: {str(e)}"
    
    return {
        "index": idx,
        "start": start,
        "end": end,
        "text": text,
        "summary": PROCESSING_SUMMARY,  # æ‘˜è¦ç¨å¾Œç”Ÿæˆ
        "volume": float(volume)
    }


async def generate_batch_summary(segments: List[Dict[str, Any]], batch_start_idx: int) -> str:
    """ç‚ºä¸€å€‹æ‰¹æ¬¡ï¼ˆ3æ®µæˆ–å‰©é¤˜æ®µè½ï¼‰ç”Ÿæˆæ‘˜è¦"""
    if not kuwa_client:
        return f"ï¼ˆæ¨¡æ“¬ï¼‰ç¬¬ {batch_start_idx} ~ {batch_start_idx + len(segments) - 1} æ®µæ‘˜è¦"
    
    # æ”¶é›†æ‰¹æ¬¡å…§çš„æœ‰æ•ˆæ–‡å­—
    batch_texts = []
    for seg in segments:
        text = seg.get("text", "").strip()
        if text and text != PROCESSING_TEXT and not text.startswith("è½‰éŒ„å¤±æ•—") and not text.startswith("ï¼ˆæ¨¡æ“¬ï¼‰"):
            batch_texts.append(text)
    
    if not batch_texts:
        return f"ç¬¬ {batch_start_idx} ~ {batch_start_idx + len(segments) - 1} æ®µæ‘˜è¦ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"
    
    try:
        combined_text = "\n".join(batch_texts)
        prompt = f"""è«‹ç°¡çŸ­æ‘˜è¦ä»¥ä¸‹ç¬¬ {batch_start_idx}-{batch_start_idx + len(segments) - 1} æ®µçš„å…§å®¹ï¼ˆ2-3å¥ï¼‰ï¼š\n{combined_text}"""
        
        message = [{"role": "user", "content": prompt}]
        
        summary_result = ""
        print(f"ğŸ“ ç”Ÿæˆç¬¬ {batch_start_idx}-{batch_start_idx + len(segments) - 1} æ®µçš„æ‰¹æ¬¡æ‘˜è¦...")
        async for chunk in kuwa_client.chat_complete(messages=message, streaming=True):
            summary_result += chunk
            
        return f"ç¬¬ {batch_start_idx} ~ {batch_start_idx + len(segments) - 1} æ®µæ‘˜è¦: {summary_result.strip()}"
        
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
        return f"ç¬¬ {batch_start_idx} ~ {batch_start_idx + len(segments) - 1} æ®µæ‘˜è¦ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"


async def generate_overall_summary(all_segments: List[Dict[str, Any]], base_name: str) -> str:
    """åŸºæ–¼æ‰¹æ¬¡æ‘˜è¦ä»£è¡¨æ®µè½ç”Ÿæˆæ•´é«”æ‘˜è¦"""
    if not kuwa_client:
        return "ï¼ˆæ¨¡æ“¬ï¼‰é€™æ˜¯åŸºæ–¼æ‰¹æ¬¡æ‘˜è¦ç”Ÿæˆçš„æ•´é«”æ‘˜è¦ã€‚"
    
    # æ‰¾å‡ºæ¯å€‹æ‰¹æ¬¡çš„æœ€å¾Œä¸€æ®µï¼ˆåŒ…å«å®Œæ•´æ‰¹æ¬¡æ‘˜è¦çš„æ®µè½ï¼‰
    batch_summaries = []
    processed_indices = set()
    
    for seg in all_segments:
        idx = seg.get("index")
        summary = seg.get("summary", "")
        
        if (idx not in processed_indices and 
            summary and 
            summary != PROCESSING_SUMMARY and 
            summary.startswith("index ") and 
            "çš„æ‘˜è¦" in summary):
            
            # è§£ææ‘˜è¦ä¸­çš„ç¯„åœï¼Œä¾‹å¦‚ "index 1 ~ 3 çš„æ‘˜è¦: ..."
            try:
                range_part = summary.split("çš„æ‘˜è¦")[0]  # "index 1 ~ 3"
                if "~" in range_part:
                    start_str, end_str = range_part.replace("index ", "").split(" ~ ")
                    start_idx = int(start_str.strip())
                    end_idx = int(end_str.strip())
                    
                    # ç¢ºä¿é€™æ˜¯æ‰¹æ¬¡çš„æœ€å¾Œä¸€æ®µ
                    if idx == end_idx:
                        batch_summaries.append(summary)
                        # æ¨™è¨˜é€™å€‹ç¯„åœçš„æ‰€æœ‰indexç‚ºå·²è™•ç†
                        for i in range(start_idx, end_idx + 1):
                            processed_indices.add(i)
            except:
                continue
    
    if not batch_summaries:
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆçš„è½‰éŒ„æ–‡å­—
        all_text = []
        for seg in all_segments:
            text = seg.get("text", "").strip()
            if text and text != PROCESSING_TEXT and not text.startswith("è½‰éŒ„å¤±æ•—"):
                all_text.append(text)
        
        if not all_text:
            return "ç„¡æœ‰æ•ˆå…§å®¹å¯ä¾›æ‘˜è¦ã€‚"
        
        combined_text = "\n".join(all_text)
    else:
        combined_text = "\n".join(batch_summaries)
    
    try:
        prompt = """
ä½ æ˜¯å°ˆæ¥­è¨˜éŒ„åˆ†æå¸«ï¼Œéœ€è¦ç”Ÿæˆå®Œæ•´çš„æœƒè­°æˆ–å…§å®¹æ‘˜è¦ã€‚
è¦å‰‡ï¼š
1. åŸºæ–¼æä¾›çš„åˆ†æ®µæ‘˜è¦ï¼Œç”Ÿæˆå®Œæ•´çš„æ•´é«”æ‘˜è¦
2. æ‘˜è¦æ‡‰åŒ…æ‹¬ï¼š
   - ä¸»è¦ä¸»é¡Œï¼ˆ1-2å¥ï¼‰
   - é—œéµå…§å®¹è¦é»ï¼ˆæ¢åˆ—å¼ï¼‰
   - é‡è¦çµè«–æˆ–æ±ºè­°ï¼ˆå¦‚æœ‰ï¼‰
3. ä½¿ç”¨ç¹é«”ä¸­æ–‡
4. ä¿æŒç°¡æ½”ä½†å®Œæ•´

è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ç”Ÿæˆæ•´é«”æ‘˜è¦ï¼š
        """
        
        message = [{"role": "user", "content": f"{prompt}\n{combined_text}"}]
        
        overall_summary = ""
        print(f"ğŸ“‹ ç”Ÿæˆ {base_name} çš„æ•´é«”æ‘˜è¦ï¼ˆåŸºæ–¼ {len(batch_summaries)} å€‹æ‰¹æ¬¡æ‘˜è¦ï¼‰...")
        async for chunk in kuwa_client.chat_complete(messages=message, streaming=True):
            overall_summary += chunk
            
        return overall_summary.strip()
        
    except Exception as e:
        print(f"âŒ æ•´é«”æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
        return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}"


def _write_json(path: Path, data: Any):
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


def _read_json(path: Path) -> Any:
    return json.loads(path.read_text(encoding="utf-8"))


def _update_segment_in_json(tr_path: Path, segment: Dict[str, Any]):
    """æ›´æ–°å–®å€‹æ®µè½åˆ° transcript.json"""
    if tr_path.exists():
        tr_data = _read_json(tr_path)
    else:
        return
    
    segments = tr_data.get("segments", [])
    idx = segment["index"]
    
    # æ‰¾åˆ°å°æ‡‰çš„æ®µè½ä¸¦æ›´æ–°
    for i, seg in enumerate(segments):
        if seg.get("index") == idx:
            segments[i] = segment
            break
    else:
        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œæ·»åŠ æ–°æ®µè½
        segments.append(segment)
        segments.sort(key=lambda x: x.get("index", 0))
    
    tr_data["segments"] = segments
    _write_json(tr_path, tr_data)


def _update_batch_summary_in_json(tr_path: Path, batch_segments: List[Dict[str, Any]], batch_summary: str):
    """æ›´æ–°æ•´å€‹æ‰¹æ¬¡çš„æ‘˜è¦åˆ° transcript.json"""
    if tr_path.exists():
        tr_data = _read_json(tr_path)
    else:
        return
    
    segments = tr_data.get("segments", [])
    
    # æ›´æ–°æ‰¹æ¬¡ä¸­æ‰€æœ‰æ®µè½çš„æ‘˜è¦
    for batch_seg in batch_segments:
        idx = batch_seg["index"]
        for i, seg in enumerate(segments):
            if seg.get("index") == idx:
                segments[i]["summary"] = batch_summary  # ç›´æ¥å¥—ç”¨æ‰¹æ¬¡æ‘˜è¦
    
    tr_data["segments"] = segments
    _write_json(tr_path, tr_data)


def _create_summary_json(folder: Path, all_segments: List[Dict[str, Any]], overall_summary: str):
    """å‰µå»ºæ–°æ ¼å¼çš„ summary.jsonï¼Œåœ¨ overall æ‘˜è¦å®Œæˆå¾ŒåŸ·è¡Œ"""
    sm_path = folder / SUMMARY_JSON
    
    # å»ºç«‹æ–°æ ¼å¼çš„ summary.json
    sm_data = {
        "base_name": folder.name,
        "chunk_seconds": CHUNK_SECONDS,
        "overlap_seconds": OVERLAP_SECONDS,
        "per_segment": [],
        "overall_summary": overall_summary
    }
    
    # ç‚ºæ¯å€‹æ®µè½å»ºç«‹ç¨ç«‹çš„æ‘˜è¦è¨˜éŒ„
    for seg in all_segments:
        idx = seg.get("index")
        summary = seg.get("summary", "")
        
        # æ ¹æ“šæ‰¹æ¬¡ä½ç½®æ±ºå®šæ‘˜è¦å…§å®¹
        if "è™•ç†ä¸­" in summary:
            # å¦‚æœé‚„åœ¨è™•ç†ä¸­ï¼Œä¿æŒè™•ç†ä¸­ç‹€æ…‹
            segment_summary = summary
        elif summary.startswith("index ") and "çš„æ‘˜è¦" in summary:
            # å¦‚æœæ˜¯æ‰¹æ¬¡æ‘˜è¦ï¼Œåˆ†æé€™å€‹æ®µè½åœ¨æ‰¹æ¬¡ä¸­çš„ä½ç½®
            try:
                range_part = summary.split("çš„æ‘˜è¦")[0]  # "index 1 ~ 3"
                if "~" in range_part:
                    start_str, end_str = range_part.replace("index ", "").split(" ~ ")
                    start_idx = int(start_str.strip())
                    end_idx = int(end_str.strip())
                    
                    if start_idx <= idx <= end_idx:
                        segment_summary = summary
                        # if idx == end_idx:
                        #     # æ‰¹æ¬¡çš„æœ€å¾Œä¸€æ®µï¼Œé¡¯ç¤ºå®Œæ•´æ‘˜è¦
                        #     segment_summary = summary
                        # else:
                        #     # æ‰¹æ¬¡ä¸­çš„å…¶ä»–æ®µè½ï¼Œé¡¯ç¤ºè™•ç†ä¸­ç‹€æ…‹
                        #     position = idx - start_idx + 1
                        #     total_in_batch = end_idx - start_idx + 1
                        #     segment_summary = f"è™•ç†ä¸­({position}/{total_in_batch})"
                    else:
                        segment_summary = summary
                else:
                    segment_summary = summary
            except:
                segment_summary = summary
        else:
            segment_summary = summary
        
        sm_data["per_segment"].append({
            "index": idx,
            "summary": segment_summary
        })
    
    # æŒ‰ index æ’åº
    sm_data["per_segment"].sort(key=lambda x: x.get("index", 0))
    
    _write_json(sm_path, sm_data)
    print(f"âœ… æ–°æ ¼å¼ summary.json å·²å»ºç«‹ï¼ŒåŒ…å« {len(sm_data['per_segment'])} å€‹æ®µè½æ‘˜è¦")


# ===== ä¸²æµæ¨¡å¼è¼”åŠ© =====
def _ensure_stream_dir(base_name: str) -> Path:
    folder = _ensure_folder(base_name)
    sdir = folder / STREAM_CHUNKS
    sdir.mkdir(parents=True, exist_ok=True)
    return sdir


def _stitch_stream_chunks_to_base(base_name: str) -> Path:
    """
    å°‡ /uploads/<base>/stream_chunks/ å…§çš„ 001.wav,002.wav... ä¾åºä¸²æ¥æˆ base.wavï¼Œ
    ä¸¦è¦ä¸€åŒ–ç‚º 16k/mono/16bitï¼Œæœ€å¾Œè¼¸å‡ºç‚º base.wavã€‚
    """
    folder = _ensure_folder(base_name)
    chunk_dir = folder / STREAM_CHUNKS
    if not chunk_dir.exists():
        raise HTTPException(status_code=400, detail="No chunks uploaded")

    files = sorted(chunk_dir.glob("*.wav"))
    if not files:
        raise HTTPException(status_code=400, detail="No chunk files found")

    merged = AudioSegment.silent(duration=0)
    for p in files:
        merged += AudioSegment.from_wav(p.as_posix())

    # è¦ä¸€åŒ–è¼¸å‡º base.wav
    full_wav = folder / FULL_WAV
    merged = merged.set_frame_rate(TARGET_SR).set_channels(TARGET_CH).set_sample_width(TARGET_BITS // 8)
    merged.export(full_wav.as_posix(), format="wav")
    return full_wav


# ===============================
# ä¸²æµæ¨¡å¼çš„å…¨åŸŸç‹€æ…‹ç®¡ç†
# ===============================
stream_states = {}  # base_name -> {"pending_segments": [], "processed_count": 0}

def _get_stream_state(base_name: str) -> Dict[str, Any]:
    if base_name not in stream_states:
        stream_states[base_name] = {
            "pending_segments": [],
            "processed_count": 0
        }
    return stream_states[base_name]


@router.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    contents = await file.read()
    if not contents:
        raise HTTPException(status_code=400, detail="Empty file uploaded")

    orig_name = file.filename
    base_name = os.path.splitext(orig_name)[0]
    folder = _ensure_folder(base_name)

    # 1) ç›´æ¥ç”± bytes ç”Ÿæˆè¦ä¸€åŒ–å®Œæ•´ WAVï¼ˆä¸å­˜åŸå§‹æª”ï¼‰
    full_wav = folder / FULL_WAV
    _canonicalize_bytes_to_wav16k_mono(contents, full_wav, os.path.splitext(orig_name)[1])

    # 2) è¨ˆç®—é ä¼°æ®µæ•¸ä¸¦åˆå§‹åŒ– JSON æª”æ¡ˆï¼ˆåªåˆå§‹åŒ– transcript.jsonï¼‰
    audio = AudioSegment.from_wav(full_wav.as_posix())
    total_ms = len(audio)
    step_ms = (CHUNK_SECONDS - OVERLAP_SECONDS) * 1000
    estimated_segments = max(1, math.ceil(total_ms / step_ms))
    
    tr_path, sm_path = _init_json_files(folder, estimated_segments)

    # 3) å‰µå»ºè‡¨æ™‚åˆ‡ç‰‡è³‡æ–™å¤¾é€²è¡Œè½‰éŒ„
    temp_chunks_dir = folder / "temp_chunks"
    temp_chunks_dir.mkdir(exist_ok=True)
    
    # æ¸…ç©ºèˆŠçš„è‡¨æ™‚æª”æ¡ˆ
    for p in temp_chunks_dir.glob("*.wav"):
        try:
            p.unlink()
        except:
            pass

    # 4) å¯¦éš›åˆ‡ç‰‡ä¸¦è½‰éŒ„ï¼Œæ¯3æ®µåšä¸€æ¬¡æ‘˜è¦
    chunk_ms = CHUNK_SECONDS * 1000
    segments = []
    pending_for_summary = []
    index = 1
    
    for start_ms in range(0, max(total_ms, 1), step_ms):
        if start_ms >= total_ms:
            break
            
        end_ms = start_ms + chunk_ms
        chunk = audio[start_ms:end_ms]
        chunk_path = temp_chunks_dir / f"{index:03d}.wav"
        chunk.export(chunk_path.as_posix(), format="wav")

        # è½‰éŒ„ç•¶å‰æ®µè½
        seg = await transcribe_with_whisper(chunk_path, index)

        # è¨­å®šæ˜ç¢ºçš„ã€Œè™•ç†ä¸­(ä½ç½®/æ‰¹æ¬¡å¤§å°)ã€ç‹€æ…‹ï¼Œé¿å…è¢«é€šç”¨ PROCESSING_SUMMARY è¦†å¯«
        position_in_batch = ((index - 1) % SUMMARY_BATCH_SIZE) + 1
        seg["summary"] = f"è™•ç†ä¸­({position_in_batch}/{SUMMARY_BATCH_SIZE})"

        segments.append(seg)
        pending_for_summary.append(seg)

        # ç«‹å³æ›´æ–°åˆ° transcript.json
        _update_segment_in_json(tr_path, seg)
        print(f"âœ… ç¬¬ {index:03d} æ®µè½‰éŒ„å®Œæˆä¸¦å·²å¯«å…¥ï¼ˆsummary: {seg['summary']}ï¼‰")
        
        # æ¯3æ®µæˆ–æœ€å¾Œä¸€æ‰¹ï¼Œç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
        if len(pending_for_summary) >= SUMMARY_BATCH_SIZE or (start_ms + chunk_ms >= total_ms):
            batch_start_idx = pending_for_summary[0]["index"]
            batch_end_idx = pending_for_summary[-1]["index"]
            
            # ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
            batch_summary_text = await generate_batch_summary(pending_for_summary, batch_start_idx)
            
            # æ›´æ–°æ‰¹æ¬¡ä¸­æ‰€æœ‰æ®µè½çš„æ‘˜è¦
            _update_batch_summary_in_json(tr_path, pending_for_summary, batch_summary_text)
            print(f"âœ… ç¬¬ {batch_start_idx}-{batch_end_idx} æ®µæ‰¹æ¬¡æ‘˜è¦å®Œæˆä¸¦å·²å¯«å…¥")
            
            # æ¸…ç©ºå¾…æ‘˜è¦åˆ—è¡¨
            pending_for_summary = []
        
        index += 1
        
        if start_ms + chunk_ms >= total_ms:
            break
    
    # 5) ç”Ÿæˆæ•´é«”æ‘˜è¦
    # âš ï¸ é‡é»ï¼šé‡æ–°è®€å– transcript.json å–å¾—æœ€æ–°çš„æ®µè½è³‡æ–™ï¼ˆå«æ‰¹æ¬¡æ‘˜è¦ï¼‰
    tr_data = _read_json(tr_path)
    segments = tr_data.get("segments", [])

    overall_summary = await generate_overall_summary(segments, base_name)

    # 6) å»ºç«‹æ–°æ ¼å¼çš„ summary.jsonï¼ˆåœ¨ overall æ‘˜è¦å®Œæˆå¾Œï¼‰
    _create_summary_json(folder, segments, overall_summary)

    # 7) æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    try:
        import shutil
        shutil.rmtree(temp_chunks_dir, ignore_errors=True)
    except Exception:
        pass

    print(f"ğŸ‰ å®Œæ•´è½‰éŒ„å®Œæˆï¼Œå…±è™•ç† {len(segments)} å€‹ç‰‡æ®µ")

    return JSONResponse({
        "filename": f"{base_name}.wav",
        "base_name": base_name,
        "status": "ok",
        "total_segments": len(segments),
        "paths": {
            "audio_url":      f"/uploads/{base_name}/{FULL_WAV}",
            "transcript_url": f"/uploads/{base_name}/{TRANSCRIPT_JSON}",
            "summary_url":    f"/uploads/{base_name}/{SUMMARY_JSON}",
        }
    })


# ===============================
# ä¸²æµå¼ï¼šæ¯æ®µç«‹å³è½‰éŒ„ï¼Œæ¯3æ®µåšä¸€æ¬¡æ‘˜è¦
# ===============================
@router.post("/ingest_chunk")
async def ingest_chunk(base_name: str = Query(...), index: int = Query(...), file: UploadFile = File(...)):
    folder = _ensure_folder(base_name)
    sdir = _ensure_stream_dir(base_name)
    contents = await file.read()
    out_wav = sdir / f"{index:03d}.wav"
    _canonicalize_bytes_to_wav16k_mono(contents, out_wav, os.path.splitext(file.filename)[1])

    # åˆå§‹åŒ– JSON æª”æ¡ˆï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ï¼Œåªåˆå§‹åŒ– transcript.jsonï¼‰
    if index == 1:
        _init_json_files(folder)

    # å–å¾—ä¸²æµç‹€æ…‹
    state = _get_stream_state(base_name)

    # è½‰éŒ„ç•¶å‰æ®µè½
    seg = await transcribe_with_whisper(out_wav, index)

    # è¨­å®šã€Œè™•ç†ä¸­(ä½ç½®/æ‰¹æ¬¡å¤§å°)ã€æ¨™è¨˜ï¼ˆé¿å…è¢«é€šç”¨å­—ä¸²è¦†è“‹ï¼‰
    position_in_batch = ((index - 1) % SUMMARY_BATCH_SIZE) + 1
    seg["summary"] = f"è™•ç†ä¸­({position_in_batch}/{SUMMARY_BATCH_SIZE})"

    state["pending_segments"].append(seg)
    state["processed_count"] += 1

    # æª¢æŸ¥æ˜¯å¦ç‚ºæ‰¹æ¬¡çš„æœ€å¾Œä¸€æ®µï¼ˆæ¯3æ®µæˆ–æœ€å¾Œä¸€æ‰¹ï¼‰
    tr_path = folder / TRANSCRIPT_JSON
    
    if len(state["pending_segments"]) >= SUMMARY_BATCH_SIZE:
        # æ‰¹æ¬¡æ»¿äº†ï¼Œè™•ç†æ‘˜è¦
        batch_segments = state["pending_segments"][:SUMMARY_BATCH_SIZE]
        state["pending_segments"] = state["pending_segments"][SUMMARY_BATCH_SIZE:]

        batch_start_idx = batch_segments[0]["index"]
        batch_end_idx = batch_segments[-1]["index"]

        # ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
        batch_summary_text = await generate_batch_summary(batch_segments, batch_start_idx)

        # æ›´æ–°æ‰¹æ¬¡æ‘˜è¦åˆ°æ‰€æœ‰æ®µè½
        for batch_seg in batch_segments:
            batch_seg["summary"] = batch_summary_text

        # ***é‡é»ä¿®æ­£ï¼šæ‰¹æ¬¡å®Œæˆå¾Œï¼Œæ‰å°‡è½‰éŒ„å’Œæ‘˜è¦ä¸€èµ·å¯«å…¥ JSON***
        for batch_seg in batch_segments:
            _update_segment_in_json(tr_path, batch_seg)
        
        print(f"âœ… ä¸²æµç¬¬ {batch_start_idx}-{batch_end_idx} æ®µæ‰¹æ¬¡å®Œæˆï¼ˆè½‰éŒ„+æ‘˜è¦ä¸€èµ·å¯«å…¥ï¼‰")
    
    else:
        # ***æ‰¹æ¬¡æœªæ»¿ï¼Œåƒ…å¯«å…¥è½‰éŒ„çµæœï¼ˆæ‘˜è¦ä¿æŒè™•ç†ä¸­ç‹€æ…‹ï¼‰***
        _update_segment_in_json(tr_path, seg)
        print(f"âœ… ä¸²æµç¬¬ {index:03d} æ®µè½‰éŒ„å®Œæˆä¸¦å·²å¯«å…¥ï¼ˆç­‰å¾…æ‰¹æ¬¡æ‘˜è¦ï¼‰")

    return seg


# ===============================
# ä¸²æµå®Œæˆï¼šè™•ç†å‰©é¤˜æ®µè½ä¸¦ç”Ÿæˆæœ€çµ‚æ‘˜è¦
# ===============================
@router.post("/finalize_stream")
async def finalize_stream(base_name: str):
    """
    åœæ­¢éŒ„éŸ³æ™‚å‘¼å«ï¼š
    1. è™•ç†å‰©é¤˜æœªæ»¿3æ®µçš„å…§å®¹
    2. ä¸²æ¥éŸ³è¨Šæª”æ¡ˆ
    3. ç”Ÿæˆæœ€çµ‚æ•´é«”æ‘˜è¦
    """
    folder = _ensure_folder(base_name)
    state = _get_stream_state(base_name)

    # 1) è™•ç†å‰©é¤˜çš„æœªæ»¿3æ®µçš„å…§å®¹
    if state["pending_segments"]:
        batch_start_idx = state["pending_segments"][0]["index"]
        batch_end_idx = state["pending_segments"][-1]["index"]
        
        # ç”Ÿæˆæœ€å¾Œä¸€æ‰¹çš„æ‘˜è¦
        batch_summary_text = await generate_batch_summary(state["pending_segments"], batch_start_idx)
        
        # ***é‡é»ä¿®æ­£ï¼šæ›´æ–°æ‘˜è¦åˆ°æ®µè½ç‰©ä»¶ä¸­ï¼Œç„¶å¾Œä¸€èµ·å¯«å…¥ JSON***
        for pending_seg in state["pending_segments"]:
            pending_seg["summary"] = batch_summary_text
        
        # å°‡è½‰éŒ„å’Œæ‘˜è¦ä¸€èµ·å¯«å…¥ JSON
        tr_path = folder / TRANSCRIPT_JSON
        for pending_seg in state["pending_segments"]:
            _update_segment_in_json(tr_path, pending_seg)
            
        print(f"âœ… æœ€çµ‚æ‰¹æ¬¡ç¬¬ {batch_start_idx}-{batch_end_idx} æ®µå®Œæˆï¼ˆè½‰éŒ„+æ‘˜è¦ä¸€èµ·å¯«å…¥ï¼Œå…± {len(state['pending_segments'])} æ®µï¼‰")
        
        # æ¸…ç©ºå¾…è™•ç†åˆ—è¡¨
        state["pending_segments"] = []

    # 2) ä¸²æ¥éŸ³è¨Šæª”æ¡ˆ
    try:
        full_wav = _stitch_stream_chunks_to_base(base_name)
        print(f"âœ… å·²å®ŒæˆéŸ³è¨Šä¸²æ¥ï¼š{full_wav}")
    except Exception as e:
        print(f"âš ï¸ éŸ³è¨Šä¸²æ¥å¤±æ•—: {e}")

    # 3) è®€å–æ‰€æœ‰è½‰éŒ„çµæœï¼Œç”Ÿæˆæœ€çµ‚æ•´é«”æ‘˜è¦
    tr_path = folder / TRANSCRIPT_JSON
    
    if tr_path.exists():
        tr_data = _read_json(tr_path)
        segments = tr_data.get("segments", [])
        
        # ç”Ÿæˆæœ€çµ‚æ•´é«”æ‘˜è¦
        overall_summary = await generate_overall_summary(segments, base_name)
        
        # 4) å»ºç«‹æ–°æ ¼å¼çš„ summary.jsonï¼ˆåœ¨ overall æ‘˜è¦å®Œæˆå¾Œï¼‰
        _create_summary_json(folder, segments, overall_summary)
        
        print(f"âœ… æœ€çµ‚æ•´é«”æ‘˜è¦å·²ç”Ÿæˆä¸¦å¯«å…¥æ–°æ ¼å¼ summary.json")

    # 5) æ¸…ç†ä¸²æµç‹€æ…‹
    if base_name in stream_states:
        del stream_states[base_name]

    # 6) æ¸…ç†ï¼šåˆªæ‰èˆŠçš„ chunks/ï¼Œä¿ç•™ stream_chunks/
    try:
        import shutil
        shutil.rmtree((folder / CHUNK_DIRNAME), ignore_errors=True)
        print("ğŸ—‘ï¸ æ¸…ç†èˆŠçš„ chunks è³‡æ–™å¤¾")
    except Exception as e:
        print(f"æ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    print(f"ğŸ‰ ä¸²æµè½‰éŒ„å®Œæˆï¼Œå…±è™•ç† {state['processed_count']} å€‹ç‰‡æ®µ")

    return JSONResponse({
        "filename": f"{base_name}.wav",
        "base_name": base_name,
        "status": "finalized",
        "total_segments": state["processed_count"],
        "paths": {
            "audio_url":      f"/uploads/{base_name}/{FULL_WAV}",
            "transcript_url": f"/uploads/{base_name}/{TRANSCRIPT_JSON}",
            "summary_url":    f"/uploads/{base_name}/{SUMMARY_JSON}",
        }
    })


# ===============================
# æŸ¥è©¢ä»‹é¢
# ===============================
@router.get("/segment_at")
async def segment_at(base_name: str = Query(...), t: float = Query(..., ge=0.0)):
    """ä¾æ™‚é–“é» tï¼ˆç§’ï¼‰å›å‚³è©²æ®µçš„è½‰éŒ„èˆ‡æ‘˜è¦ã€‚"""
    folder = _ensure_folder(base_name)
    tr_path = folder / TRANSCRIPT_JSON
    if not tr_path.exists():
        raise HTTPException(status_code=404, detail="Transcript not found. Please transcribe first.")

    tr = _read_json(tr_path)
    segs: List[Dict[str, Any]] = tr.get("segments", [])
    total_seconds = segs[-1]["end"] if segs else 0.0
    idx = _time_to_index(t, total_seconds)

    if 1 <= idx <= len(segs):
        return segs[idx - 1]
    else:
        raise HTTPException(status_code=404, detail="Time out of transcript range")


@router.get("/segments_in_range")
async def segments_in_range(
    base_name: str = Query(...),
    start: float = Query(..., ge=0.0),
    end: float = Query(..., gt=0.0)
):
    """ä¾æ™‚é–“ç¯„åœ [start, end) å›å‚³è¦†è“‹åˆ°çš„æ‰€æœ‰æ®µè½ï¼ˆå·²å«é‡ç–Šï¼‰ã€‚"""
    if end <= start:
        raise HTTPException(status_code=400, detail="end must be greater than start")

    folder = _ensure_folder(base_name)
    tr_path = folder / TRANSCRIPT_JSON
    if not tr_path.exists():
        raise HTTPException(status_code=404, detail="Transcript not found. Please transcribe first.")

    tr = _read_json(tr_path)
    segs: List[Dict[str, Any]] = tr.get("segments", [])

    hit: List[Dict[str, Any]] = []
    for s in segs:
        s0, s1 = float(s["start"]), float(s["end"])
        if not (s1 <= start or s0 >= end):
            hit.append(s)

    return {"base_name": base_name, "range": [start, end], "segments": hit}


@router.get("/summary")
async def get_summary(base_name: str = Query(...)):
    """å–å›æ•´é«”æ‘˜è¦ï¼ˆä»¥åŠå¯é¸çš„é€æ®µæ‘˜è¦ï¼‰ã€‚"""
    folder = _ensure_folder(base_name)
    sp = folder / SUMMARY_JSON
    if not sp.exists():
        raise HTTPException(status_code=404, detail="Summary not found. Please transcribe first.")
    return _read_json(sp)


@router.get("/status")
async def get_status(base_name: str = Query(...)):
    """å–å¾—è½‰éŒ„å’Œæ‘˜è¦çš„é€²åº¦ç‹€æ…‹"""
    folder = _ensure_folder(base_name)
    tr_path = folder / TRANSCRIPT_JSON
    sm_path = folder / SUMMARY_JSON
    
    status_info = {
        "base_name": base_name,
        "transcript_exists": tr_path.exists(),
        "summary_exists": sm_path.exists(),
        "total_segments": 0,
        "completed_transcripts": 0,
        "processing_summaries": 0,
        "completed_summaries": 0
    }
    
    if tr_path.exists():
        tr_data = _read_json(tr_path)
        segments = tr_data.get("segments", [])
        status_info["total_segments"] = len(segments)
        
        # è¨ˆç®—å®Œæˆçš„è½‰éŒ„å’Œæ‘˜è¦æ•¸é‡
        completed_transcripts = 0
        processing_summaries = 0
        completed_summaries = 0
        
        for seg in segments:
            text = seg.get("text", "")
            summary = seg.get("summary", "")
            
            # è¨ˆç®—è½‰éŒ„å®Œæˆæ•¸
            if text and text != PROCESSING_TEXT:
                completed_transcripts += 1
            
            # è¨ˆç®—æ‘˜è¦ç‹€æ…‹
            if "è™•ç†ä¸­" in summary:
                processing_summaries += 1
            elif "index " in summary and "çš„æ‘˜è¦" in summary:
                completed_summaries += 1
        
        status_info["completed_transcripts"] = completed_transcripts
        status_info["processing_summaries"] = processing_summaries
        status_info["completed_summaries"] = completed_summaries
    
    return status_info


@router.get("/model_status")
async def model_status():
    """æª¢æŸ¥ AI æ¨¡å‹ç‹€æ…‹"""
    return {
        "whisper_loaded": whisper_app is not None,
        "kuwa_client_ready": kuwa_client is not None,
        "status": "ready" if (whisper_app and kuwa_client) else "partial" if (whisper_app or kuwa_client) else "simulation_mode",
        "summary_batch_size": SUMMARY_BATCH_SIZE,
        "chunk_seconds": CHUNK_SECONDS,
        "overlap_seconds": OVERLAP_SECONDS
    }


# ===============================
# é‡æ–°è™•ç†æ‘˜è¦çš„è¼”åŠ©ç«¯é»ï¼ˆå¯é¸ï¼‰
# ===============================
@router.post("/regenerate_summaries")
async def regenerate_summaries(base_name: str = Query(...)):
    """é‡æ–°ç”Ÿæˆæ‰€æœ‰æ‘˜è¦ï¼ˆåŸºæ–¼ç¾æœ‰çš„è½‰éŒ„çµæœï¼‰"""
    folder = _ensure_folder(base_name)
    tr_path = folder / TRANSCRIPT_JSON
    
    if not tr_path.exists():
        raise HTTPException(status_code=404, detail="No transcript found")
    
    tr_data = _read_json(tr_path)
    segments = tr_data.get("segments", [])
    
    if not segments:
        raise HTTPException(status_code=400, detail="No segments found")
    
    # é‡æ–°ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
    for i in range(0, len(segments), SUMMARY_BATCH_SIZE):
        batch_segments = segments[i:i + SUMMARY_BATCH_SIZE]
        batch_start_idx = batch_segments[0]["index"]
        
        # ç”Ÿæˆæ‰¹æ¬¡æ‘˜è¦
        batch_summary_text = await generate_batch_summary(batch_segments, batch_start_idx)
        
        # æ›´æ–°æ‰¹æ¬¡ä¸­æ‰€æœ‰æ®µè½çš„æ‘˜è¦
        _update_batch_summary_in_json(tr_path, batch_segments, batch_summary_text)
    
    # é‡æ–°è®€å–æ›´æ–°å¾Œçš„è³‡æ–™
    tr_data = _read_json(tr_path)
    updated_segments = tr_data.get("segments", [])
    
    # ç”Ÿæˆæ•´é«”æ‘˜è¦
    overall_summary = await generate_overall_summary(updated_segments, base_name)
    
    # å»ºç«‹æ–°æ ¼å¼çš„ summary.json
    _create_summary_json(folder, updated_segments, overall_summary)
    
    return JSONResponse({
        "base_name": base_name,
        "status": "regenerated",
        "total_segments": len(segments)
    })